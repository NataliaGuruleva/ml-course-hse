{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Qp0H_zUQuu_"
   },
   "source": [
    "# Нейронные сети\n",
    "__Суммарное количество баллов: 10__\n",
    "\n",
    "Для начала вам предстоит реализовать свой собственный backpropagation и протестировать его на реальных данных, а затем научиться обучать нейронные сети при помощи библиотеки `PyTorch` и использовать это умение для классификации классического набора данных CIFAR10.\n",
    "\n",
    "Обратите внимание, что использование PyTorch во всех заданиях кроме последнего запрещено. Автоматической проверки на его использование не будет, однако все посылки будут проверены вручную. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "22ezVRf3QuvA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from typing import List, NoReturn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_{ij}$, $i$ - номер батча, $j$ - номер признака\n",
    "$\\begin{pmatrix}\n",
    "x_{11} & x_{12} & x_{13} \\\\\n",
    "x_{21} & x_{22} & x_{23}\n",
    "\\end{pmatrix} \\cdot \n",
    "\\begin{pmatrix}\n",
    "w_{11} & w_{12} & w_{13} \\\\\n",
    "w_{21} & w_{22} & w_{23}\n",
    "\\end{pmatrix}^T=\n",
    "\\begin{pmatrix}\n",
    "x_{11}w_{11} + x_{12}w_{12} + x_{13}w_{13} & x_{11}w_{21} + x_{12}w_{22} + x_{13}w_{23} \\\\\n",
    "x_{21}w_{11} + x_{22}w_{12} + x_{23}w_{13} & x_{21}w_{21} + x_{22}w_{22} + x_{23}w_{23}\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "y_{11} & y_{12} \\\\\n",
    "y_{21} & y_{22}\n",
    "\\end{pmatrix}$\n",
    "$\\\\$ Известно $\\dfrac{dL}{dy}$, хотим найти $\\dfrac{dL}{dx}$:\n",
    "$\\begin{pmatrix}\n",
    "\\dfrac{dL}{dx_{11}} & \\dfrac{dL}{dx_{12}} & \\dfrac{dL}{dx_{13}} \\\\\n",
    "\\dfrac{dL}{dx_{21}} & \\dfrac{dL}{dx_{22}} & \\dfrac{dL}{dx_{23}}\n",
    "\\end{pmatrix}$\n",
    "$\\\\\\\\ \\dfrac{dL}{dx_{11}}=\\dfrac{dL}{dy_{11}}\\dfrac{dy_{11}}{dx_{11}}+\\dfrac{dL}{dy_{12}}\\dfrac{dy_{12}}{dx_{11}}=\\dfrac{dL}{dy_{11}}w_{11}+\\dfrac{dL}{dy_{12}}w_{21}$\n",
    "$\\\\\\\\ \\dfrac{dL}{dx_{12}}=\\dfrac{dL}{dy_{11}}\\dfrac{dy_{11}}{dx_{12}}+\\dfrac{dL}{dy_{12}}\\dfrac{dy_{12}}{dx_{12}}=\\dfrac{dL}{dy_{11}}w_{12}+\\dfrac{dL}{dy_{12}}w_{22}$\n",
    "$\\\\\\\\ \\dfrac{dL}{dx_{13}}=\\dfrac{dL}{dy_{11}}\\dfrac{dy_{11}}{dx_{13}}+\\dfrac{dL}{dy_{12}}\\dfrac{dy_{12}}{dx_{13}}=\\dfrac{dL}{dy_{11}}w_{13}+\\dfrac{dL}{dy_{12}}w_{23}$\n",
    "$\\\\\\\\ \\dfrac{dL}{dx_{21}}=\\dfrac{dL}{dy_{21}}\\dfrac{dy_{21}}{dx_{21}}+\\dfrac{dL}{dy_{22}}\\dfrac{dy_{22}}{dx_{21}}=\\dfrac{dL}{dy_{21}}w_{11}+\\dfrac{dL}{dy_{22}}w_{21}$\n",
    "$\\\\\\\\ \\dfrac{dL}{dx_{22}}=\\dfrac{dL}{dy_{21}}\\dfrac{dy_{21}}{dx_{22}}+\\dfrac{dL}{dy_{22}}\\dfrac{dy_{22}}{dx_{22}}=\\dfrac{dL}{dy_{21}}w_{12}+\\dfrac{dL}{dy_{22}}w_{22}$\n",
    "$\\\\\\\\ \\dfrac{dL}{dx_{23}}=\\dfrac{dL}{dy_{21}}\\dfrac{dy_{21}}{dx_{23}}+\\dfrac{dL}{dy_{22}}\\dfrac{dy_{22}}{dx_{23}}=\\dfrac{dL}{dy_{21}}w_{13}+\\dfrac{dL}{dy_{22}}w_{23}$\n",
    "$\\\\\\\\ \\dfrac{dL}{dx}=\n",
    "\\begin{pmatrix}\n",
    "\\dfrac{dL}{dy_{11}} & \\dfrac{dL}{dy_{12}} \\\\\n",
    "\\dfrac{dL}{dy_{21}} & \\dfrac{dL}{dy_{22}}\n",
    "\\end{pmatrix} \\cdot\n",
    "\\begin{pmatrix}\n",
    "w_{11} & w_{12} & w_{13} \\\\\n",
    "w_{21} & w_{22} & w_{23}\n",
    "\\end{pmatrix}=\\dfrac{dL}{dy} \\cdot W$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\\\$ Известно $\\dfrac{dL}{dy}$, хотим найти $\\dfrac{dL}{dw}$:\n",
    "$\\\\\\\\ \\dfrac{dL}{dw_{11}}=\\dfrac{dL}{dy_{11}}\\dfrac{dy_{11}}{dw_{11}}+\\dfrac{dL}{dy_{12}}\\dfrac{dy_{12}}{dw_{11}}+\\dfrac{dL}{dy_{21}}\\dfrac{dy_{21}}{dw_{11}}+\\dfrac{dL}{dy_{22}}\\dfrac{dy_{22}}{dw_{11}}=\\dfrac{dL}{dy_{11}}x_{11}+\\dfrac{dL}{dy_{21}}x_{21}$\n",
    "$\\\\\\\\ \\dfrac{dL}{dw_{12}}=\\dfrac{dL}{dy_{11}}x_{12}+\\dfrac{dL}{dy_{21}}x_{22}$\n",
    "$\\\\\\\\ \\dfrac{dL}{dw_{13}}=\\dfrac{dL}{dy_{11}}x_{13}+\\dfrac{dL}{dy_{21}}x_{23}$\n",
    "$\\\\\\\\ \\dfrac{dL}{dw_{21}}=\\dfrac{dL}{dy_{12}}x_{11}+\\dfrac{dL}{dy_{22}}x_{21}$\n",
    "$\\\\\\\\ \\dfrac{dL}{dw_{22}}=\\dfrac{dL}{dy_{12}}x_{12}+\\dfrac{dL}{dy_{22}}x_{22}$\n",
    "$\\\\\\\\ \\dfrac{dL}{dw_{23}}=\\dfrac{dL}{dy_{12}}x_{13}+\\dfrac{dL}{dy_{22}}x_{23}$\n",
    "$\\dfrac{dL}{dW}=\n",
    "\\begin{pmatrix}\n",
    "\\dfrac{dL}{dw_{11}} & \\dfrac{dL}{dw_{12}} & \\dfrac{dL}{dw_{13}} \\\\\n",
    "\\dfrac{dL}{dw_{21}} & \\dfrac{dL}{dw_{22}} & \\dfrac{dL}{dw_{23}}\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "\\dfrac{dL}{dy_{11}} & \\dfrac{dL}{dy_{21}} \\\\\n",
    "\\dfrac{dL}{dy_{12}} & \\dfrac{dL}{dy_{22}}\n",
    "\\end{pmatrix} \\cdot\n",
    "\\begin{pmatrix}\n",
    "x_{11} & x_{12} & x_{13} \\\\\n",
    "x_{21} & x_{22} & x_{23}\n",
    "\\end{pmatrix}=\\dfrac{dL}{dy}^T\\cdot X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\\\$ Известно $\\dfrac{dL}{dy}$, хотим найти $\\dfrac{dL}{db}$:\n",
    "$\\dfrac{dL}{db}=\n",
    "\\begin{pmatrix}\n",
    "\\dfrac{dL}{db_1} &\\dfrac{dL}{db_2}\n",
    "\\end{pmatrix}\n",
    "\\\\\\\\ \\dfrac{dL}{db_1}=\\dfrac{dL}{dy_{11}}\\dfrac{dy_{11}}{db_1}+\\dfrac{dL}{dy_{21}}\\dfrac{dy_{21}}{db_1}=\\dfrac{dL}{dy_{11}}+\\dfrac{dL}{dy_{21}}\n",
    "\\\\\\\\ \\dfrac{dL}{db_2}=\\dfrac{dL}{dy_{12}}\\dfrac{dy_{12}}{db_2}+\\dfrac{dL}{dy_{22}}\\dfrac{dy_{22}}{db_2}=\\dfrac{dL}{dy_{12}}+\\dfrac{dL}{dy_{22}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qfDPH_LQuvF"
   },
   "source": [
    "### Задание 1 (3 балла)\n",
    "Нейронные сети состоят из слоев, поэтому для начала понадобится реализовать их. Пока нам понадобятся только три:\n",
    "\n",
    "`Linear` - полносвязный слой, в котором `y = Wx + b`, где `y` - выход, `x` - вход, `W` - матрица весов, а `b` - смещение. \n",
    "\n",
    "`ReLU` - слой, соответствующий функции активации `y = max(0, x)`.\n",
    "\n",
    "\n",
    "#### Методы\n",
    "`forward(X)` - возвращает предсказанные для `X`. `X` может быть как вектором, так и батчем\n",
    "\n",
    "`backward(d)` - считает градиент при помощи обратного распространения ошибки. Возвращает новое значение `d`\n",
    "\n",
    "`update(alpha)` - обновляет веса (если необходимо) с заданой скоростью обучения\n",
    "\n",
    "#### Оценка\n",
    "Валидируется корректность работы каждого модуля отдельно. Ожидается, что выходы каждого модуля будут незначительно отличаться от ожидаемых выходов, а подсчет градиента и градиентный спуск будут работать корректно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aYS2gE4PYepZ"
   },
   "outputs": [],
   "source": [
    "from task import ReLU, Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rb_ip_h8QuvJ"
   },
   "source": [
    "### Задание 2 (2 балла)\n",
    "Теперь сделаем саму нейронную сеть.\n",
    "\n",
    "#### Методы\n",
    "`fit(X, y)` - обучает нейронную сеть заданное число эпох. В каждой эпохе необходимо использовать [cross-entropy loss](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy) для обучения, а так же производить обновления не по одному элементу, а используя батчи.\n",
    "\n",
    "`predict_proba(X)` - предсказывает вероятности классов для элементов `X`\n",
    "\n",
    "#### Параметры конструктора\n",
    "`modules` - список, состоящий из ранее реализованных модулей и описывающий слои нейронной сети. В конец необходимо добавить `Softmax`\n",
    "\n",
    "`epochs` - количество эпох обучения\n",
    "\n",
    "`alpha` - скорость обучения\n",
    "\n",
    "#### Оценка\n",
    "Оценка производится на заданных ботом гиперпараметрах и архитектурах. Ожидается, что при подобранных заранее гиперпараметрах решение будет демонстрировать приемлемую точность.\n",
    "\n",
    "Всего 20 тестов по 500 точек в обучающей выборке и по 100 точек в тестовой выборке c 20 эпохами обучения и 10 тестов по 1000 точек в обучающей выборке и 200 точек в тестовой выборке с 40 эпохами обучения. Количество признаков варьируется от 2 до 8. Количество классов не более 8 и не менее 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Q_JFCizKQuvK"
   },
   "outputs": [],
   "source": [
    "from task import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "onDymYQXQuvN"
   },
   "outputs": [],
   "source": [
    "p = MLPClassifier([\n",
    "    Linear(4, 8),\n",
    "    ReLU(),\n",
    "    Linear(8, 8),\n",
    "    ReLU(),\n",
    "    Linear(8, 2)\n",
    "])\n",
    "\n",
    "X = np.random.randn(50, 4)\n",
    "y = np.array([(0 if x[0] > x[2]**2 or x[3]**3 > 0.5 else 1) for x in X])\n",
    "p.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3C1EIsDqQuvQ"
   },
   "source": [
    "### Задание 3 (2 балла)\n",
    "Протестируем наше решение на синтетических данных. Необходимо подобрать гиперпараметры, при которых качество полученных классификаторов будет достаточным.\n",
    "\n",
    "Первый датасет - датасет moons. В каждом тесте у данных всего два признака, классов также два.\n",
    "\n",
    "Второй датасет - датасет blobs. В каждом тесте у данных по два признака, классов три.\n",
    "\n",
    "\n",
    "Обратите внимание, что датасеты могут отличаться от приведенных ниже по количеству точек, уровню шума и положению центроидов. Количество классов и признаков остается неизменным.\n",
    "\n",
    "Обратите внимание, что классификатор будет обучаться ботом под каждый датасет отдельно. Обучать самостоятельно в файле `task.py` классификатор не нужно.\n",
    "\n",
    "Количество датасетов каждого типа равно 5. Количество точек в обучающей выборке не менее 1000, количество точек в тестовой выборке не менее 200.\n",
    "\n",
    "#### Оценка\n",
    "Средняя точность на датасетах moons больше 0.85 - +1 балл\n",
    "\n",
    "Средняя точность на датасетах blobs больше 0.85 - +1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task import classifier_moons, classifier_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "d5UAgXTcQuvQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "X, y = make_moons(400, noise=0.075)\n",
    "X_test, y_test = make_moons(400, noise=0.075)\n",
    "classifier_moons.fit(X, y)\n",
    "print(\"Accuracy\", np.mean(classifier_moons.predict(X_test) == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MMDJM4qFQuvT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9475\n"
     ]
    }
   ],
   "source": [
    "X, y = make_blobs(400, 2, centers=[[0, 0], [2.5, 2.5], [-2.5, 3]])\n",
    "X_test, y_test = make_blobs(400, 2, centers=[[0, 0], [2.5, 2.5], [-2.5, 3]])\n",
    "classifier_blobs.fit(X, y)\n",
    "print(\"Accuracy\", np.mean(classifier_blobs.predict(X_test) == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPbVTFnMQuvW"
   },
   "source": [
    "## PyTorch\n",
    "\n",
    "Для выполнения следующего задания понадобится PyTorch. [Инструкция по установке](https://pytorch.org/get-started/locally/)\n",
    "\n",
    "Если у вас нет GPU, то можно использовать [Google Colab](https://colab.research.google.com/) или обучать сеть на CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tV0mJLu-QuvX"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VUC_QqpAQuva"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "t = transforms.ToTensor()\n",
    "\n",
    "cifar_train = datasets.CIFAR10(\"datasets/cifar10\", download=True, train=True, transform=t)\n",
    "train_loader = DataLoader(cifar_train, batch_size=1024, shuffle=True, pin_memory=torch.cuda.is_available())\n",
    "cifar_test = datasets.CIFAR10(\"datasets/cifar10\", download=True, train=False, transform=t)\n",
    "test_loader = DataLoader(cifar_test, batch_size=1024, shuffle=False, pin_memory=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGmpjcFfQuvd"
   },
   "source": [
    "### Задание 4 (3 балла)\n",
    "А теперь поработам с настоящими нейронными сетями и настоящими данными. Необходимо реализовать сверточную нейронную сеть, которая будет классифицировать изображения из датасета CIFAR10. Имплементируйте класс `Model` и функцию `calculate_loss`. \n",
    "\n",
    "Обратите внимание, что `Model` должна считать в конце `softmax`, т.к. мы решаем задачу классификации. Соответствеено, функция `calculate_loss` считает cross-entropy.\n",
    "\n",
    "Для успешного выполнения задания необходимо, чтобы `accuracy`, `mean precision` и `mean recall` были больше 0.5\n",
    "\n",
    "__Можно пользоваться всем содержимым библиотеки PyTorch.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5sRmTKwKQuve"
   },
   "outputs": [],
   "source": [
    "from task import TorchModel, calculate_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAsLmkUqQuvh"
   },
   "source": [
    "Теперь обучим нашу модель. Для этого используем ранее созданные batch loader'ы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "k5G8iMCeQuvh"
   },
   "outputs": [],
   "source": [
    "def train(model, epochs=100):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for i in range(epochs):\n",
    "        #Train\n",
    "        loss_mean = 0\n",
    "        elements = 0\n",
    "        for X, y in iter(train_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            loss = calculate_loss(X, y, model)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_mean += loss.item() * len(X)\n",
    "            elements += len(X)\n",
    "        train_losses.append(loss_mean / elements)\n",
    "        #Test\n",
    "        loss_mean = 0 \n",
    "        elements = 0\n",
    "        for X, y in iter(test_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            loss = calculate_loss(X, y, model)\n",
    "            loss_mean += loss.item() * len(X)\n",
    "            elements += len(X)\n",
    "        test_losses.append(loss_mean / elements)\n",
    "        print(\"Epoch\", i, \"| Train loss\", train_losses[-1], \"| Test loss\", test_losses[-1])\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "vmD9eWJOQuvl",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train loss 2.211659017410278 | Test loss 2.149003789138794\n",
      "Epoch 1 | Train loss 2.1193999309539793 | Test loss 2.097071643066406\n",
      "Epoch 2 | Train loss 2.0899573742675783 | Test loss 2.073940073776245\n",
      "Epoch 3 | Train loss 2.0704240088653565 | Test loss 2.0588814239501954\n",
      "Epoch 4 | Train loss 2.055748277359009 | Test loss 2.047711050796509\n",
      "Epoch 5 | Train loss 2.04816930480957 | Test loss 2.037094931793213\n",
      "Epoch 6 | Train loss 2.0369272689819335 | Test loss 2.038382763671875\n",
      "Epoch 7 | Train loss 2.0292246574401855 | Test loss 2.027954956817627\n",
      "Epoch 8 | Train loss 2.0227835085296633 | Test loss 2.024086897277832\n",
      "Epoch 9 | Train loss 2.015681316757202 | Test loss 2.0198336322784423\n",
      "Epoch 10 | Train loss 2.008187232131958 | Test loss 2.0097593269348146\n",
      "Epoch 11 | Train loss 2.003947477684021 | Test loss 2.009067084121704\n",
      "Epoch 12 | Train loss 1.9959874710083008 | Test loss 2.003058681488037\n",
      "Epoch 13 | Train loss 1.987865350341797 | Test loss 1.9984518547058105\n",
      "Epoch 14 | Train loss 1.9825069552230834 | Test loss 1.9926413635253906\n",
      "Epoch 15 | Train loss 1.976127102470398 | Test loss 1.9823253171920776\n",
      "Epoch 16 | Train loss 1.9660154580688476 | Test loss 1.9769545083999633\n",
      "Epoch 17 | Train loss 1.9566571170806886 | Test loss 1.9734434818267823\n",
      "Epoch 18 | Train loss 1.9511361904907227 | Test loss 1.971060885810852\n",
      "Epoch 19 | Train loss 1.9471677920532227 | Test loss 1.9624028535842895\n",
      "Epoch 20 | Train loss 1.9382930739212036 | Test loss 1.9636146089553832\n",
      "Epoch 21 | Train loss 1.9385285955047606 | Test loss 1.9614112668991088\n",
      "Epoch 22 | Train loss 1.9318018243789672 | Test loss 1.9618282936096192\n",
      "Epoch 23 | Train loss 1.9300437007141114 | Test loss 1.9501846029281615\n",
      "Epoch 24 | Train loss 1.9250478838348388 | Test loss 1.9557452070236205\n",
      "Epoch 25 | Train loss 1.9220500968170167 | Test loss 1.9484901947021485\n",
      "Epoch 26 | Train loss 1.9178216048431396 | Test loss 1.950363292503357\n",
      "Epoch 27 | Train loss 1.9148766820526124 | Test loss 1.9475947149276733\n",
      "Epoch 28 | Train loss 1.9131841263198852 | Test loss 1.9453125631332397\n",
      "Epoch 29 | Train loss 1.910573925704956 | Test loss 1.9421225063323975\n",
      "Epoch 30 | Train loss 1.9062372805404664 | Test loss 1.9397644241333007\n",
      "Epoch 31 | Train loss 1.9043028495025636 | Test loss 1.9389705278396607\n",
      "Epoch 32 | Train loss 1.9027912498092652 | Test loss 1.9376475345611572\n",
      "Epoch 33 | Train loss 1.899195940361023 | Test loss 1.9465818464279174\n",
      "Epoch 34 | Train loss 1.8964215375900269 | Test loss 1.9357912721633912\n",
      "Epoch 35 | Train loss 1.8946932690811158 | Test loss 1.936413597869873\n",
      "Epoch 36 | Train loss 1.8901249145889283 | Test loss 1.9363537954330445\n",
      "Epoch 37 | Train loss 1.8891194187927247 | Test loss 1.936438816833496\n",
      "Epoch 38 | Train loss 1.8865975785446167 | Test loss 1.935927811050415\n",
      "Epoch 39 | Train loss 1.8878808959579467 | Test loss 1.9339147869110107\n",
      "Epoch 40 | Train loss 1.8824873512268065 | Test loss 1.9325349124908446\n",
      "Epoch 41 | Train loss 1.8832860914993286 | Test loss 1.9308330337524413\n",
      "Epoch 42 | Train loss 1.8775904611968994 | Test loss 1.9287999961853028\n",
      "Epoch 43 | Train loss 1.8780921483993531 | Test loss 1.9359124677658082\n",
      "Epoch 44 | Train loss 1.8789124198532106 | Test loss 1.9309804353713989\n",
      "Epoch 45 | Train loss 1.8742535957717896 | Test loss 1.933244065475464\n",
      "Epoch 46 | Train loss 1.8741466008377075 | Test loss 1.9305720306396483\n",
      "Epoch 47 | Train loss 1.872594734802246 | Test loss 1.929035860824585\n",
      "Epoch 48 | Train loss 1.8729803874206543 | Test loss 1.9311871208190918\n",
      "Epoch 49 | Train loss 1.8716056130218506 | Test loss 1.929695280265808\n",
      "Epoch 50 | Train loss 1.8650303191375732 | Test loss 1.9265315814971924\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m TorchModel()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 2\u001b[0m train_l, test_l \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, epochs)\u001b[0m\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m calculate_loss(X, y, model)\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     16\u001b[0m loss_mean \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = TorchModel().to(device)\n",
    "train_l, test_l = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJNAuHjNQuvn"
   },
   "source": [
    "Построим график функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "F6OEGqriQuvo"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_l\u001b[49m)), train_l, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_l)), test_l, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_l' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(train_l)), train_l, label=\"train\")\n",
    "plt.plot(range(len(test_l)), test_l, label=\"test\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miUxg0bDQuvs"
   },
   "source": [
    "И, наконец, посчитаем метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "UXSOJFI8Quvt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy 0.5241\n",
      "Precision [0.53754266 0.63958917 0.41457587 0.36900369 0.42666667 0.49416342\n",
      " 0.64332036 0.50705329 0.6087344  0.6381323 ]\n",
      "Recall [0.63  0.685 0.347 0.4   0.48  0.381 0.496 0.647 0.683 0.492]\n",
      "Mean Precision 0.527878183140649\n",
      "Mean Recall 0.5241\n"
     ]
    }
   ],
   "source": [
    "true_positive = np.zeros(10)\n",
    "true_negative = np.zeros(10)\n",
    "false_positive = np.zeros(10)\n",
    "false_negative = np.zeros(10)\n",
    "accuracy = 0\n",
    "ctn = 0\n",
    "for X, y in iter(test_loader):\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X).max(dim=1)[1]\n",
    "    for i in range(10):\n",
    "        for pred, real in zip(y_pred, y):\n",
    "            if real == i:\n",
    "                if pred == real:\n",
    "                    true_positive[i] += 1\n",
    "                else:\n",
    "                    false_negative[i] += 1\n",
    "            else:\n",
    "                if pred == i:\n",
    "                    false_positive[i] += 1\n",
    "                else:\n",
    "                    true_negative[i] += 1\n",
    "            \n",
    "    accuracy += torch.sum(y_pred == y).item()\n",
    "    ctn += len(y)\n",
    "print(\"Overall accuracy\", accuracy / ctn)\n",
    "print(\"Precision\", true_positive / (true_positive + false_positive))\n",
    "print(\"Recall\", true_positive / (true_positive + false_negative))\n",
    "print(\"Mean Precision\", np.mean(true_positive / (true_positive + false_positive)))\n",
    "print(\"Mean Recall\", np.mean(true_positive / (true_positive + false_negative)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKA-j4rIQuvv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "hw06_task.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
